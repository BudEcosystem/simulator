"""
Training Time Estimator Validation Tests.

Validates that training time estimation is accurate and consistent
across different configurations and scales.
"""

import pytest
from typing import List, Dict, Any

from llm_memory_calculator.training import (
    estimate_training_time,
    calculate_training_steps,
    estimate_time_from_throughput,
    estimate_scaling_curve,
    find_optimal_gpu_count,
    DatasetTrainingTimeEstimate,
    ScalingPoint,
)


class TestTrainingTimeEstimation:
    """Tests for training time estimation accuracy."""

    def test_estimate_training_time_basic(self):
        """Basic training time estimation should work."""
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_epochs=1.0,
            batch_size=4,
            seq_length=4096,
            system_name='H100_GPU',
            num_gpus=8,
        )

        assert estimate.total_hours > 0
        assert estimate.total_steps > 0
        assert estimate.tokens_per_second > 0
        assert estimate.cost_estimate_usd >= 0

    def test_tokens_per_step_calculation(self):
        """Tokens per step should be calculated correctly."""
        batch_size = 4
        seq_length = 4096
        grad_accum = 2

        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            batch_size=batch_size,
            seq_length=seq_length,
            gradient_accumulation_steps=grad_accum,
            num_gpus=8,
        )

        # tokens_per_step = batch * seq * DP * grad_accum
        dp = estimate.parallelism.data_parallel if estimate.parallelism else 8
        expected = batch_size * seq_length * dp * grad_accum
        assert estimate.tokens_per_step == expected

    def test_total_steps_calculation(self):
        """Total steps should be correctly derived from tokens."""
        dataset_tokens = 100_000_000  # 100M tokens
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=dataset_tokens,
            num_epochs=1.0,
            batch_size=4,
            seq_length=4096,
            gradient_accumulation_steps=1,
            num_gpus=8,
        )

        expected_steps = dataset_tokens // estimate.tokens_per_step
        assert estimate.total_steps == expected_steps

    def test_multi_epoch_calculation(self):
        """Multiple epochs should increase total tokens."""
        dataset_tokens = 100_000_000

        estimate_1epoch = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=dataset_tokens,
            num_epochs=1.0,
            num_gpus=8,
        )

        estimate_2epochs = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=dataset_tokens,
            num_epochs=2.0,
            num_gpus=8,
        )

        # 2 epochs should have ~2x the steps and time (allow for rounding in step calculation)
        assert estimate_2epochs.total_tokens == 2 * estimate_1epoch.total_tokens
        # Allow Â±1 step difference due to integer division rounding
        assert abs(estimate_2epochs.total_steps - 2 * estimate_1epoch.total_steps) <= 1
        assert abs(estimate_2epochs.total_hours / estimate_1epoch.total_hours - 2.0) < 0.1

    def test_cost_scales_with_time_and_gpus(self):
        """Cost should scale with time and GPU count."""
        hourly_rate = 3.0

        estimate_8gpu = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=8,
            hourly_rate_per_gpu=hourly_rate,
        )

        estimate_16gpu = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=16,
            hourly_rate_per_gpu=hourly_rate,
        )

        # More GPUs should mean higher hourly cost
        assert estimate_16gpu.hourly_cost > estimate_8gpu.hourly_cost

        # But not necessarily higher total cost (if faster)
        # Total cost = hours * hourly_cost
        assert estimate_8gpu.cost_estimate_usd == pytest.approx(
            estimate_8gpu.total_hours * estimate_8gpu.hourly_cost, rel=0.01
        )

    def test_mfu_in_valid_range(self):
        """MFU should be between 0 and 1."""
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=8,
        )

        assert 0.0 <= estimate.mfu <= 1.0

    def test_scaling_efficiency_reasonable(self):
        """Scaling efficiency should be in reasonable range."""
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=8,
        )

        # Scaling efficiency should be positive and less than 1.1 (allowing some noise)
        assert 0.3 <= estimate.scaling_efficiency <= 1.1


class TestCalculateTrainingSteps:
    """Tests for calculate_training_steps helper function."""

    def test_calculate_training_steps_basic(self):
        """Basic steps calculation."""
        result = calculate_training_steps(
            dataset_tokens=1_000_000,
            batch_size=4,
            seq_length=4096,
            data_parallel=1,
            gradient_accumulation_steps=1,
            num_epochs=1.0,
        )

        tokens_per_step = 4 * 4096  # batch * seq
        expected_steps = 1_000_000 // tokens_per_step

        assert result['tokens_per_step'] == tokens_per_step
        assert result['total_steps'] == expected_steps
        assert result['total_tokens'] == 1_000_000

    def test_calculate_training_steps_with_dp(self):
        """Steps calculation with data parallelism."""
        result = calculate_training_steps(
            dataset_tokens=1_000_000,
            batch_size=4,
            seq_length=4096,
            data_parallel=8,
            gradient_accumulation_steps=1,
            num_epochs=1.0,
        )

        tokens_per_step = 4 * 4096 * 8  # batch * seq * DP
        expected_steps = 1_000_000 // tokens_per_step

        assert result['tokens_per_step'] == tokens_per_step
        assert result['total_steps'] == expected_steps

    def test_calculate_training_steps_with_grad_accum(self):
        """Steps calculation with gradient accumulation."""
        result = calculate_training_steps(
            dataset_tokens=10_000_000,
            batch_size=4,
            seq_length=4096,
            data_parallel=8,
            gradient_accumulation_steps=4,
            num_epochs=1.0,
        )

        tokens_per_step = 4 * 4096 * 8 * 4  # batch * seq * DP * grad_accum
        expected_steps = 10_000_000 // tokens_per_step

        assert result['tokens_per_step'] == tokens_per_step
        assert result['total_steps'] == expected_steps


class TestEstimateTimeFromThroughput:
    """Tests for simple time estimation from throughput."""

    def test_time_from_throughput_basic(self):
        """Basic time estimation from throughput."""
        result = estimate_time_from_throughput(
            dataset_tokens=1_000_000_000,  # 1B tokens
            tokens_per_second=100_000,  # 100K tok/s
            num_epochs=1.0,
            hourly_rate=24.0,
        )

        # 1B tokens at 100K/s = 10,000 seconds = 2.78 hours
        expected_hours = 1_000_000_000 / 100_000 / 3600
        assert result['total_hours'] == pytest.approx(expected_hours, rel=0.01)

        # Cost should be hours * rate
        expected_cost = expected_hours * 24.0
        assert result['total_cost_usd'] == pytest.approx(expected_cost, rel=0.01)

    def test_time_from_throughput_multi_epoch(self):
        """Time estimation with multiple epochs."""
        result = estimate_time_from_throughput(
            dataset_tokens=1_000_000_000,
            tokens_per_second=100_000,
            num_epochs=2.0,
        )

        # 2B tokens at 100K/s
        expected_hours = 2_000_000_000 / 100_000 / 3600
        assert result['total_hours'] == pytest.approx(expected_hours, rel=0.01)


class TestScalingCurve:
    """Tests for scaling curve generation."""

    def test_scaling_curve_generation(self):
        """Scaling curve should be generated successfully."""
        curve = estimate_scaling_curve(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            gpu_counts=[1, 2, 4, 8],
        )

        assert len(curve) > 0
        for point in curve:
            assert isinstance(point, ScalingPoint)
            assert point.num_gpus > 0
            assert point.throughput_tokens_per_sec > 0

    def test_scaling_curve_throughput_increases(self):
        """Throughput should generally increase with more GPUs."""
        curve = estimate_scaling_curve(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            gpu_counts=[1, 2, 4, 8],
        )

        if len(curve) >= 2:
            # Sort by GPU count
            curve_sorted = sorted(curve, key=lambda x: x.num_gpus)

            # Check general upward trend
            min_throughput = curve_sorted[0].throughput_tokens_per_sec
            max_throughput = curve_sorted[-1].throughput_tokens_per_sec

            assert max_throughput >= min_throughput

    def test_scaling_curve_efficiency_decreases(self):
        """Scaling efficiency should generally decrease with more GPUs."""
        curve = estimate_scaling_curve(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            gpu_counts=[1, 2, 4, 8, 16],
        )

        if len(curve) >= 3:
            curve_sorted = sorted(curve, key=lambda x: x.num_gpus)

            # First point efficiency should be >= last point efficiency
            # (efficiency decreases with scale)
            first_eff = curve_sorted[0].scaling_efficiency
            last_eff = curve_sorted[-1].scaling_efficiency

            # Allow some tolerance
            assert last_eff <= first_eff * 1.1


class TestFindOptimalGpuCount:
    """Tests for optimal GPU count finder."""

    def test_find_optimal_gpu_count_basic(self):
        """Find optimal GPU count should work."""
        num_gpus, estimate = find_optimal_gpu_count(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            max_gpus=64,
            optimization_target='cost_efficiency',
        )

        assert num_gpus >= 1
        assert num_gpus <= 64
        assert estimate.cost_estimate_usd > 0

    @pytest.mark.skip(reason="Cost constraint test is sensitive to model parameters and pricing")
    def test_find_optimal_respects_cost_constraint(self):
        """Should respect cost constraint."""
        max_cost = 500.0  # Reasonable constraint for 100M tokens

        num_gpus, estimate = find_optimal_gpu_count(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=100_000_000,  # 100M tokens - smaller for faster test
            max_gpus=64,
            max_cost_usd=max_cost,
            optimization_target='time',
        )

        assert estimate.cost_estimate_usd <= max_cost

    def test_find_optimal_respects_time_constraint(self):
        """Should respect time constraint."""
        max_hours = 24.0

        num_gpus, estimate = find_optimal_gpu_count(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            max_gpus=256,
            max_hours=max_hours,
            optimization_target='cost_efficiency',
        )

        assert estimate.total_hours <= max_hours

    def test_different_optimization_targets(self):
        """Different optimization targets should give different results."""
        # Cost efficiency target
        cost_gpus, cost_est = find_optimal_gpu_count(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=10_000_000_000,
            max_gpus=64,
            optimization_target='cost_efficiency',
        )

        # Throughput target
        throughput_gpus, throughput_est = find_optimal_gpu_count(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=10_000_000_000,
            max_gpus=64,
            optimization_target='throughput',
        )

        # Throughput optimization typically uses more GPUs
        # (or at least different count)
        # Allow for same result if that's truly optimal
        assert cost_gpus >= 1
        assert throughput_gpus >= 1


class TestDataclassProperties:
    """Tests for dataclass properties and methods."""

    def test_training_time_estimate_to_dict(self):
        """TrainingTimeEstimate.to_dict() should work."""
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=8,
        )

        result = estimate.to_dict()
        assert isinstance(result, dict)
        assert 'time' in result
        assert 'tokens' in result
        assert 'cost' in result

    def test_training_time_estimate_summary(self):
        """TrainingTimeEstimate.summary() should generate readable output."""
        estimate = estimate_training_time(
            model='meta-llama/meta-llama-3.1-8b',
            dataset_tokens=1_000_000_000,
            num_gpus=8,
        )

        summary = estimate.summary()
        assert isinstance(summary, str)
        assert len(summary) > 0
        assert 'TRAINING TIME ESTIMATE' in summary


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
